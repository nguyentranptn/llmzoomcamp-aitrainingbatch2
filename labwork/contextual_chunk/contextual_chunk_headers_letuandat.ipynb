{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDxBqG_qfDfz"
      },
      "source": [
        "# Contextual Chunk Headers (CCH)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Contextual chunk headers (CCH) is a method of creating chunk headers that contain higher-level context (such as document-level or section-level context), and prepending those chunk headers to the chunks prior to embedding them. This gives the embeddings a much more accurate and complete representation of the content and meaning of the text. In our testing, this feature leads to a substantial improvement in retrieval quality. In addition to increasing the rate at which the correct information is retrieved, CCH also reduces the rate at which irrelevant results show up in the search results. This reduces the rate at which the LLM misinterprets a piece of text in downstream chat and generation applications.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Many of the problems developers face with RAG come down to this: Individual chunks oftentimes do not contain sufficient context to be properly used by the retrieval system or the LLM. This leads to the inability to answer questions and, more worryingly, hallucinations.\n",
        "\n",
        "Examples of this problem\n",
        "- Chunks oftentimes refer to their subject via implicit references and pronouns. This causes them to not be retrieved when they should be, or to not be properly understood by the LLM.\n",
        "- Individual chunks oftentimes only make sense in the context of the entire section or document, and can be misleading when read on their own.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "#### Contextual chunk headers\n",
        "The idea here is to add in higher-level context to the chunk by prepending a chunk header. This chunk header could be as simple as just the document title, or it could use a combination of document title, a concise document summary, and the full hierarchy of section and sub-section titles.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "#### Context generation\n",
        "In the demonstration below we use an LLM to generate a descriptive title for the document. This is done through a simple prompt where you pass in a truncated version of the document text and ask the LLM to generate a descriptive title for the document. If you already have sufficiently descriptive document titles then you can directly use those instead. We've found that a document title is the simplest and most important kind of higher-level context to include in the chunk header.\n",
        "\n",
        "Other kinds of context you can include in the chunk header:\n",
        "- Concise document summary\n",
        "- Section/sub-section title(s)\n",
        "    - This helps the retrieval system handle queries for larger sections or topics in documents.\n",
        "\n",
        "#### Embed chunks with chunk headers\n",
        "The text you embed for each chunk is simply the concatenation of the chunk header and the chunk text. If you use a reranker during retrieval, you'll want to make sure you use this same concatenation there too.\n",
        "\n",
        "#### Add chunk headers to search results\n",
        "Including the chunk headers when presenting the search results to the LLM is also beneficial as it gives the LLM more context, and makes it less likely that it misunderstands the meaning of a chunk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riy7HWy8fDf2"
      },
      "source": [
        "![Your Technique Name](https://github.com/NirDiamant/RAG_Techniques/blob/main/images/contextual_chunk_headers.svg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6MnFjPfDf2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "You'll need a Cohere API key and an OpenAI API key for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tbyVRtfDf3"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6GGGGEsfDf3"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain openai python-dotenv tiktoken langchain_groq cohere"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_text_splitters"
      ],
      "metadata": {
        "id": "GGhU-us5giY2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DiYPZKD2fDf4"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import tiktoken\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "os.environ[\"CO_API_KEY\"] = \"\" # Cohere API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\" # OpenAI API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fhjCu-4fDf4"
      },
      "source": [
        "## Load the document and split it into chunks\n",
        "We'll use the basic LangChain RecursiveCharacterTextSplitter for this demo, but you can combine CCH with more sophisticated chunking methods for even better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ouIZnEfDf5",
        "outputId": "2fe09b08-88be-45d9-ed11-614e2044d5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-22 02:27:45--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "data/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2026-01-22 02:27:46 (13.9 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2026-01-22 02:27:46--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/nike_2023_annual_report.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375757 (367K) [text/plain]\n",
            "Saving to: ‘data/nike_2023_annual_report.txt’\n",
            "\n",
            "data/nike_2023_annu 100%[===================>] 366.95K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-22 02:27:46 (17.3 MB/s) - ‘data/nike_2023_annual_report.txt’ saved [375757/375757]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/nike_2023_annual_report.txt https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/nike_2023_annual_report.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pfMq-LBqfDf5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_into_chunks(text: str, chunk_size: int = 800) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a given text into chunks of specified size using RecursiveCharacterTextSplitter.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be split into chunks.\n",
        "        chunk_size (int, optional): The maximum size of each chunk. Defaults to 800.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of text chunks.\n",
        "\n",
        "    Example:\n",
        "        >>> text = \"This is a sample text to be split into chunks.\"\n",
        "        >>> chunks = split_into_chunks(text, chunk_size=10)\n",
        "        >>> print(chunks)\n",
        "        ['This is a', 'sample', 'text to', 'be split', 'into', 'chunks.']\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=0,\n",
        "        length_function=len\n",
        "    )\n",
        "    documents = text_splitter.create_documents([text])\n",
        "    return [document.page_content for document in documents]\n",
        "\n",
        "# File path for the input document\n",
        "FILE_PATH = \"data/nike_2023_annual_report.txt\"\n",
        "\n",
        "# Read the document and split it into chunks\n",
        "with open(FILE_PATH, \"r\") as file:\n",
        "    document_text = file.read()\n",
        "\n",
        "chunks = split_into_chunks(document_text, chunk_size=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSY4ypRHfDf5"
      },
      "source": [
        "## Generate descriptive document title to use in chunk header"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
        "INSTRUCTIONS\n",
        "What is the title of the following document?\n",
        "\n",
        "Your response MUST be the title of the document, and nothing else. DO NOT respond with anything else.\n",
        "\n",
        "{document_title_guidance}\n",
        "\n",
        "{truncation_message}\n",
        "\n",
        "DOCUMENT\n",
        "{document_text}\n",
        "\"\"\".strip()\n",
        "\n",
        "TRUNCATION_MESSAGE = \"\"\"\n",
        "Also note that the document text provided below is just the first ~{num_words} words of the document. That should be plenty for this task. Your response should still pertain to the entire document, not just the text provided below.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "z4HWe3WAipMt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
      ],
      "metadata": {
        "id": "KLahk29Situl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTENT_TOKENS = 4000\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "def make_llm_call(chat_messages: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Make an API call using ChatGroq (LangChain wrapper).\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatGroq(\n",
        "        groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        model_name=MODEL_NAME,\n",
        "        temperature=0.2,\n",
        "        max_tokens=MAX_CONTENT_TOKENS,\n",
        "    )\n",
        "\n",
        "    # Convert OpenAI-style messages -> LangChain messages\n",
        "    lc_messages = []\n",
        "    for msg in chat_messages:\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "\n",
        "        if role == \"system\":\n",
        "            lc_messages.append(SystemMessage(content=content))\n",
        "        elif role == \"user\":\n",
        "            lc_messages.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            lc_messages.append(AIMessage(content=content))\n",
        "\n",
        "    response = llm.invoke(lc_messages)\n",
        "    return response.content.strip()\n"
      ],
      "metadata": {
        "id": "-ZEmeC9pjZDL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "chat_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain what Retrieval-Augmented Generation (RAG) is in simple terms.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = make_llm_call(chat_messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwFSrsj3ki2W",
        "outputId": "3896d5e8-5c5c-4428-8588-beada3dc7837"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) is a technique used in artificial intelligence, particularly in natural language processing. In simple terms, RAG is a way for AI models to generate text by combining two main steps:\n",
            "\n",
            "1. **Retrieval**: The AI model searches a large database or knowledge base to find relevant information related to the topic or question it's trying to answer.\n",
            "2. **Generation**: The AI model uses the retrieved information to generate a response, such as a paragraph or a sentence.\n",
            "\n",
            "Think of it like a researcher writing a paper. First, they search for relevant information in books and articles (retrieval). Then, they use that information to write their own text (generation). RAG works in a similar way, but instead of a human researcher, it's an AI model that's doing the searching and writing.\n",
            "\n",
            "The goal of RAG is to improve the accuracy and coherence of generated text by providing the AI model with relevant context and information. This can be especially useful for tasks like answering questions, summarizing documents, or generating creative content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_content(\n",
        "    text: str,\n",
        "    max_tokens: int = 400,\n",
        "    chars_per_token: float = 4.0,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Truncate text approximately by tokens without using OpenAI tokenizer.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "        max_tokens (int): Max token budget\n",
        "        chars_per_token (float): Approximation (English ~4, Vietnamese ~3–4)\n",
        "\n",
        "    Returns:\n",
        "        str: Truncated text\n",
        "    \"\"\"\n",
        "    max_chars = int(max_tokens * chars_per_token)\n",
        "\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "\n",
        "    return text[:max_chars]"
      ],
      "metadata": {
        "id": "XqpRxo7LkqnC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truncate_content(\"\"\"Retrieval-Augmented Generation (RAG) is a technique used in artificial intelligence, particularly in natural language processing. In simple terms, RAG is a way for AI models to generate text by combining two main steps:\n",
        "\n",
        "1. **Retrieval**: The AI model searches a large database or knowledge base to find relevant information related to the topic or question it's trying to answer.\n",
        "2. **Generation**: The AI model uses the retrieved information to generate a response, such as a paragraph or a sentence.\n",
        "\n",
        "Think of it like a researcher writing a paper. First, they search for relevant information in books and articles (retrieval). Then, they use that information to write their own text (generation). RAG works in a similar way, but instead of a human researcher, it's an AI model that's doing the searching and writing.\n",
        "\n",
        "The goal of RAG is to improve the accuracy and coherence of generated text by providing the AI model with relevant context and information. This can be especially useful for tasks like answering questions, summarizing documents, or generating creative content.\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Ql6h_Pofk6Ca",
        "outputId": "63a69822-f0a7-4a8b-f54d-3cc76229d341"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Retrieval-Augmented Generation (RAG) is a technique used in artificial intelligence, particularly in natural language processing. In simple terms, RAG is a way for AI models to generate text by combining two main steps:\\n\\n1. **Retrieval**: The AI model searches a large database or knowledge base to find relevant information related to the topic or question it's trying to answer.\\n2. **Generation**: The AI model uses the retrieved information to generate a response, such as a paragraph or a sentence.\\n\\nThink of it like a researcher writing a paper. First, they search for relevant information in books and articles (retrieval). Then, they use that information to write their own text (generation). RAG works in a similar way, but instead of a human researcher, it's an AI model that's doing the searching and writing.\\n\\nThe goal of RAG is to improve the accuracy and coherence of generated text by providing the AI model with relevant context and information. This can be especially useful for tasks like answering questions, summarizing documents, or generating creative content.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ioa01RLNfDf5",
        "outputId": "312d5d7e-4232-4e6d-f7ab-6306a9fe1100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Title: Nike Inc Annual Report 2023\n"
          ]
        }
      ],
      "source": [
        "def get_document_title(\n",
        "    text: str,\n",
        "    llm,\n",
        "    max_chars: int = 1500\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate a concise document title using LLM (ChatGroq).\n",
        "    \"\"\"\n",
        "\n",
        "    if not text or not text.strip():\n",
        "        return \"Untitled Document\"\n",
        "\n",
        "    text = text.strip()[:max_chars]\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an assistant that generates concise, informative document titles. \"\n",
        "                \"Return ONLY the title. No explanations, no punctuation at the end.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Generate a short title (max 10 words) for the following document:\\n\\n\"\n",
        "                f\"{text}\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        title = make_llm_call(messages)\n",
        "        return title.strip().strip('\"')\n",
        "    except Exception as e:\n",
        "        return \"Untitled Document\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    llm = ChatGroq(\n",
        "        groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        model_name=MODEL_NAME,\n",
        "        temperature=0.0,\n",
        "        max_tokens=MAX_CONTENT_TOKENS,\n",
        "    )\n",
        "    # Assuming document_text is defined elsewhere\n",
        "    document_text_1 = \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) is a technique used in artificial intelligence, particularly in natural language processing. In simple terms, RAG is a way for AI models to generate text by combining two main steps:\n",
        "\n",
        "1. **Retrieval**: The AI model searches a large database or knowledge base to find relevant information related to the topic or question it's trying to answer.\n",
        "2. **Generation**: The AI model uses the retrieved information to generate a response, such as a paragraph or a sentence.\n",
        "\n",
        "Think of it like a researcher writing a paper. First, they search for relevant information in books and articles (retrieval). Then, they use that information to write their own text (generation). RAG works in a similar way, but instead of a human researcher, it's an AI model that's doing the searching and writing.\n",
        "\n",
        "The goal of RAG is to improve the accuracy and coherence of generated text by providing the AI model with relevant context and information. This can be especially useful for tasks like answering questions, summarizing documents, or generating creative content.\n",
        "    \"\"\"\n",
        "    document_title = get_document_title(document_text, llm)\n",
        "    print(f\"Document Title: {document_title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ9k16_TfDf6"
      },
      "source": [
        "## Add chunk header and measure impact\n",
        "Let's look at a specific example to demonstrate the impact of adding a chunk header. We'll use the Cohere reranker to measure relevance to a query with and without a chunk header."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_documents(query: str, chunks: List[str]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Use Cohere Rerank API to rerank the search results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        chunks (List[str]): List of document chunks to be reranked.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of similarity scores for each chunk, in the original order.\n",
        "    \"\"\"\n",
        "    MODEL = \"rerank-multilingual-v3.0\"\n",
        "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
        "\n",
        "    reranked_results = client.rerank(model=MODEL, query=query, documents=chunks)\n",
        "    results = reranked_results.results\n",
        "    reranked_indices = [result.index for result in results]\n",
        "    reranked_similarity_scores = [result.relevance_score for result in results]\n",
        "\n",
        "    # Convert back to order of original documents\n",
        "    similarity_scores = [0] * len(chunks)\n",
        "    for i, index in enumerate(reranked_indices):\n",
        "        similarity_scores[index] = reranked_similarity_scores[i]\n",
        "\n",
        "    return similarity_scores"
      ],
      "metadata": {
        "id": "7Q6XYpo1mgWE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBeO3OcBfDf6",
        "outputId": "cb996173-5f7d-4f72-ec11-bef698da6329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunk header:\n",
            "Document Title: Nike Inc Annual Report 2023\n",
            "\n",
            "Chunk text:\n",
            "Given the broad and global scope of our operations, we are particularly vulnerable to the physical risks of climate change, such \n",
            "as shifts in weather patterns. Extreme weather conditions in the areas in which our retail stores, suppliers, manufacturers, \n",
            "customers, distribution centers, offices, headquarters and vendors are located could adversely affect our operating results and \n",
            "financial condition. Moreover, natural disasters such as earthquakes, hurricanes, wildfires, tsunamis, floods or droughts, whether \n",
            "occurring in the United States or abroad, and their related consequences and effects, including energy shortages and public \n",
            "health issues, have in the past temporarily disrupted, and could in the future disrupt, our operations, the operations of our\n",
            "\n",
            "Query: Nike climate change impact\n",
            "\n",
            "Similarity without contextual chunk header: 0.0099\n",
            "Similarity with contextual chunk header: 0.9983\n"
          ]
        }
      ],
      "source": [
        "def compare_chunk_similarities(chunk_index: int, chunks: List[str], document_title: str, query: str) -> None:\n",
        "    \"\"\"\n",
        "    Compare similarity scores for a chunk with and without a contextual header.\n",
        "\n",
        "    Args:\n",
        "        chunk_index (int): Index of the chunk to inspect.\n",
        "        chunks (List[str]): List of all document chunks.\n",
        "        document_title (str): Title of the document.\n",
        "        query (str): The search query to use for comparison.\n",
        "\n",
        "    Prints:\n",
        "        Chunk header, chunk text, query, and similarity scores with and without the header.\n",
        "    \"\"\"\n",
        "    chunk_text = chunks[chunk_index]\n",
        "    chunk_wo_header = chunk_text\n",
        "    chunk_w_header = f\"Document Title: {document_title}\\n\\n{chunk_text}\"\n",
        "\n",
        "    similarity_scores = rerank_documents(query, [chunk_wo_header, chunk_w_header])\n",
        "\n",
        "    print(f\"\\nChunk header:\\nDocument Title: {document_title}\")\n",
        "    print(f\"\\nChunk text:\\n{chunk_text}\")\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nSimilarity without contextual chunk header: {similarity_scores[0]:.4f}\")\n",
        "    print(f\"Similarity with contextual chunk header: {similarity_scores[1]:.4f}\")\n",
        "\n",
        "# Notebook cell for execution\n",
        "# Assuming chunks and document_title are defined in previous cells\n",
        "CHUNK_INDEX_TO_INSPECT = 86\n",
        "QUERY = \"Nike climate change impact\"\n",
        "\n",
        "compare_chunk_similarities(CHUNK_INDEX_TO_INSPECT, chunks, document_title, QUERY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy16UgOofDf7"
      },
      "source": [
        "This chunk is clearly about the impact of climate change on some organization, but it doesn't explicitly say \"Nike\" in it. So the relevance to the query \"Nike climate change impact\" in only about 0.1. By simply adding the document title to the chunk that similarity goes up to 0.92."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LThL_wg6fDf7"
      },
      "source": [
        "# Eval results\n",
        "\n",
        "#### KITE\n",
        "\n",
        "We evaluated CCH on an end-to-end RAG benchmark we created, called KITE (Knowledge-Intensive Task Evaluation).\n",
        "\n",
        "KITE currently consists of 4 datasets and a total of 50 questions.\n",
        "- **AI Papers** - ~100 academic papers about AI and RAG, downloaded from arXiv in PDF form.\n",
        "- **BVP Cloud 10-Ks** - 10-Ks for all companies in the Bessemer Cloud Index (~70 of them), in PDF form.\n",
        "- **Sourcegraph Company Handbook** - ~800 markdown files, with their original directory structure, downloaded from Sourcegraph's publicly accessible company handbook GitHub [page](https://github.com/sourcegraph/handbook/tree/main/content).\n",
        "- **Supreme Court Opinions** - All Supreme Court opinions from Term Year 2022 (delivered from January '23 to June '23), downloaded from the official Supreme Court [website](https://www.supremecourt.gov/opinions/slipopinion/22) in PDF form.\n",
        "\n",
        "Ground truth answers are included with each sample. Most samples also include grading rubrics. Grading is done on a scale of 0-10 for each question, with a strong LLM doing the grading.\n",
        "\n",
        "We compare performance with and without CCH. For the CCH config we use document title and document summary. All other parameters remain the same between the two configurations. We use the Cohere 3 reranker, and we use GPT-4o for response generation.\n",
        "\n",
        "|                         | No-CCH   | CCH          |\n",
        "|-------------------------|----------|--------------|\n",
        "| AI Papers               | 4.5      | 4.7          |\n",
        "| BVP Cloud               | 2.6      | 6.3          |\n",
        "| Sourcegraph             | 5.7      | 5.8          |\n",
        "| Supreme Court Opinions  | 6.1      | 7.4          |\n",
        "| **Average**             | 4.72     | 6.04         |\n",
        "\n",
        "We can see that CCH leads to an improvement in performance on each of the four datasets. Some datasets see a large improvement while others see a small improvement. The overall average score increases from 4.72 -> 6.04, a 27.9% increase.\n",
        "\n",
        "#### FinanceBench\n",
        "\n",
        "We've also evaluated CCH on FinanceBench, where it contributed to a score of 83%, compared to a baseline score of 19%. For that benchmark, we tested CCH and relevant segment extraction (RSE) jointly, so we can't say exactly how much CCH contributed to that result. But the combination of CCH and RSE clearly leads to substantial accuracy improvements on FinanceBench."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbcy4Xs6fDf7"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--contextual-chunk-headers)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "44d0561a9d33f22b2e67e0485c48036e39d1c698628b030a9859974b559ff507"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}