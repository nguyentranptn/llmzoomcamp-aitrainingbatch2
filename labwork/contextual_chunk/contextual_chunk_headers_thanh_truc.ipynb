{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai python-dotenv tiktoken cohere langchain_text_splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3kRLQgXVj7MC",
        "outputId": "0efff126-ef52-4bc6-bb54-38387ead7088"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.20.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langchain_text_splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pydantic-core>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere) (2.41.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from cohere) (0.22.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.6)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<1,>=0.15->cohere) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Downloading cohere-5.20.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, fastavro, cohere, langchain_text_splitters\n",
            "Successfully installed cohere-5.20.2 fastavro-1.12.1 langchain_text_splitters-1.1.0 types-requests-2.32.4.20260107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "import tiktoken\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from google.colab import userdata # Use for secret key in collab\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "# load_dotenv()\n",
        "# os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY') # Cohere API key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key\n",
        "# os.environ[\"OPENAI_API_BASE_URL\"] = os.getenv('OPENAI_API_BASE_URL') # OpenAI API base URL\n",
        "\n",
        "# Load environment variables from userdata\n",
        "os.environ[\"CO_API_KEY\"] = userdata.get('CO_API_KEY') # Cohere API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') # OpenAI API key\n",
        "os.environ[\"OPENAI_API_BASE_URL\"] = userdata.get('OPENAI_API_BASE_URL') # OpenAI API base URL"
      ],
      "metadata": {
        "id": "67am3JOvkRFL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/nike_2023_annual_report.txt https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/nike_2023_annual_report.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FEu-TVKgm0bh",
        "outputId": "94f0d242-518a-423c-d508-fb85241fc127"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-26 14:53:59--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "\r          data/Unde   0%[                    ]       0  --.-KB/s               \rdata/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-26 14:53:59 (11.2 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2026-01-26 14:53:59--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/nike_2023_annual_report.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375757 (367K) [text/plain]\n",
            "Saving to: ‘data/nike_2023_annual_report.txt’\n",
            "\n",
            "data/nike_2023_annu 100%[===================>] 366.95K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-01-26 14:53:59 (14.2 MB/s) - ‘data/nike_2023_annual_report.txt’ saved [375757/375757]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Splits a given text into chunks using RecursiveCharacterTextSplitter.\n",
        "def split_into_chunks(text: str, chunk_size: int = 800) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a given text into chunks of specified size using RecursiveCharacterTextSplitter.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be split into chunks.\n",
        "        chunk_size (int, optional): The maximum size of each chunk. Defaults to 800.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of text chunks.\n",
        "\n",
        "    Example:\n",
        "        >>> text = \"This is a sample text to be split into chunks.\"\n",
        "        >>> chunks = split_into_chunks(text, chunk_size=10)\n",
        "        >>> print(chunks)\n",
        "        ['This is a', 'sample', 'text to', 'be split', 'into', 'chunks.']\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=0,\n",
        "        length_function=len\n",
        "    )\n",
        "    documents = text_splitter.create_documents([text])\n",
        "    return [document.page_content for document in documents]\n",
        "\n",
        "# File path for the input document\n",
        "FILE_PATH = \"data/nike_2023_annual_report.txt\"\n",
        "\n",
        "# Read the document and split it into chunks\n",
        "with open(FILE_PATH, \"r\") as file:\n",
        "    document_text = file.read()\n",
        "\n",
        "chunks = split_into_chunks(document_text, chunk_size=800)"
      ],
      "metadata": {
        "id": "fBfRBaprnrpP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
        "INSTRUCTIONS\n",
        "What is the title of the following document?\n",
        "\n",
        "Your response MUST be the title of the document, and nothing else. DO NOT respond with anything else.\n",
        "\n",
        "{document_title_guidance}\n",
        "\n",
        "{truncation_message}\n",
        "\n",
        "DOCUMENT\n",
        "{document_text}\n",
        "\"\"\".strip()\n",
        "\n",
        "TRUNCATION_MESSAGE = \"\"\"\n",
        "Also note that the document text provided below is just the first ~{num_words} words of the document. That should be plenty for this task. Your response should still pertain to the entire document, not just the text provided below.\n",
        "\"\"\".strip()\n",
        "\n",
        "MAX_CONTENT_TOKENS = 4000\n",
        "MODEL_NAME = \"gpt-4.1\"\n",
        "TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "\n",
        "# Makes an API call to the OpenAI language model.\n",
        "def make_llm_call(chat_messages: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Make an API call to the OpenAI language model.\n",
        "\n",
        "    Args:\n",
        "        chat_messages (list[dict]): A list of message dictionaries for the chat completion.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the language model.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_API_BASE_URL\"))\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=chat_messages,\n",
        "        max_tokens=MAX_CONTENT_TOKENS,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Truncates content to a specified maximum number of tokens.\n",
        "def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:\n",
        "    \"\"\"\n",
        "    Truncate the content to a specified maximum number of tokens.\n",
        "\n",
        "    Args:\n",
        "        content (str): The input text to be truncated.\n",
        "        max_tokens (int): The maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tuple[str, int]: A tuple containing the truncated content and the number of tokens.\n",
        "    \"\"\"\n",
        "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
        "    truncated_tokens = tokens[:max_tokens]\n",
        "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
        "\n",
        "# Extracts the title of a document using a language model.\n",
        "def get_document_title(document_text: str, document_title_guidance: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Extract the title of a document using a language model.\n",
        "\n",
        "    Args:\n",
        "        document_text (str): The text of the document.\n",
        "        document_title_guidance (str, optional): Additional guidance for title extraction. Defaults to \"\".\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted document title.\n",
        "    \"\"\"\n",
        "    # Truncate the content if it's too long\n",
        "    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)\n",
        "    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else \"\"\n",
        "\n",
        "    # Prepare the prompt for title extraction\n",
        "    prompt = DOCUMENT_TITLE_PROMPT.format(\n",
        "        document_title_guidance=document_title_guidance,\n",
        "        document_text=document_text,\n",
        "        truncation_message=truncation_message\n",
        "    )\n",
        "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    return make_llm_call(chat_messages)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming document_text is defined elsewhere\n",
        "    document_title = get_document_title(document_text)\n",
        "    print(f\"Document Title: {document_title}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKJx8S7BoBbe",
        "outputId": "24474101-56b6-4260-ead2-5285664081d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Title: NIKE, Inc. Annual Report on Form 10-K for the fiscal year ended May 31, 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses Cohere Rerank API to rerank search results.\n",
        "def rerank_documents(query: str, chunks: List[str]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Use Cohere Rerank API to rerank the search results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        chunks (List[str]): List of document chunks to be reranked.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of similarity scores for each chunk, in the original order.\n",
        "    \"\"\"\n",
        "    MODEL = \"rerank-english-v3.0\"\n",
        "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
        "\n",
        "    reranked_results = client.rerank(model=MODEL, query=query, documents=chunks)\n",
        "    results = reranked_results.results\n",
        "    reranked_indices = [result.index for result in results]\n",
        "    reranked_similarity_scores = [result.relevance_score for result in results]\n",
        "\n",
        "    # Convert back to order of original documents\n",
        "    similarity_scores = [0] * len(chunks)\n",
        "    for i, index in enumerate(reranked_indices):\n",
        "        similarity_scores[index] = reranked_similarity_scores[i]\n",
        "\n",
        "    return similarity_scores\n",
        "\n",
        "# Compares similarity scores for a chunk with and without a contextual header.\n",
        "def compare_chunk_similarities(chunk_index: int, chunks: List[str], document_title: str, query: str) -> None:\n",
        "    \"\"\"\n",
        "    Compare similarity scores for a chunk with and without a contextual header.\n",
        "\n",
        "    Args:\n",
        "        chunk_index (int): Index of the chunk to inspect.\n",
        "        chunks (List[str]): List of all document chunks.\n",
        "        document_title (str): Title of the document.\n",
        "        query (str): The search query to use for comparison.\n",
        "\n",
        "    Prints:\n",
        "        Chunk header, chunk text, query, and similarity scores with and without the header.\n",
        "    \"\"\"\n",
        "    chunk_text = chunks[chunk_index]\n",
        "    chunk_wo_header = chunk_text\n",
        "    chunk_w_header = f\"Document Title: {document_title}\\n\\n{chunk_text}\"\n",
        "\n",
        "    similarity_scores = rerank_documents(query, [chunk_wo_header, chunk_w_header])\n",
        "\n",
        "    print(f\"\\nChunk header:\\nDocument Title: {document_title}\")\n",
        "    print(f\"\\nChunk text:\\n{chunk_text}\")\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nSimilarity without contextual chunk header: {similarity_scores[0]:.4f}\")\n",
        "    print(f\"Similarity with contextual chunk header: {similarity_scores[1]:.4f}\")\n",
        "\n",
        "# Notebook cell for execution\n",
        "# Assuming chunks and document_title are defined in previous cells\n",
        "CHUNK_INDEX_TO_INSPECT = 86\n",
        "QUERY = \"Nike climate change impact\"\n",
        "\n",
        "compare_chunk_similarities(CHUNK_INDEX_TO_INSPECT, chunks, document_title, QUERY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szFRZa7SoPyO",
        "outputId": "9108d9ad-ad72-4bdb-dd44-ebee604287fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunk header:\n",
            "Document Title: NIKE, Inc. Annual Report on Form 10-K for the fiscal year ended May 31, 2023\n",
            "\n",
            "Chunk text:\n",
            "Given the broad and global scope of our operations, we are particularly vulnerable to the physical risks of climate change, such \n",
            "as shifts in weather patterns. Extreme weather conditions in the areas in which our retail stores, suppliers, manufacturers, \n",
            "customers, distribution centers, offices, headquarters and vendors are located could adversely affect our operating results and \n",
            "financial condition. Moreover, natural disasters such as earthquakes, hurricanes, wildfires, tsunamis, floods or droughts, whether \n",
            "occurring in the United States or abroad, and their related consequences and effects, including energy shortages and public \n",
            "health issues, have in the past temporarily disrupted, and could in the future disrupt, our operations, the operations of our\n",
            "\n",
            "Query: Nike climate change impact\n",
            "\n",
            "Similarity without contextual chunk header: 0.1058\n",
            "Similarity with contextual chunk header: 0.7974\n"
          ]
        }
      ]
    }
  ]
}