{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN2itei52_kN"
      },
      "source": [
        "# Reranking Methods in RAG Systems\n",
        "\n",
        "Origin:\n",
        "https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnUi6Och2_kR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/reranking-visualization.svg?raw=1\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvwOY1wt2_kR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/reranking_comparison.svg?raw=1\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZdnRaiY2_kS"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  \"numpy<2.0\" \\\n",
        "  \"langchain==0.1.1\" \\\n",
        "  \"langchain-core<0.2.0\" \\\n",
        "  \"langchain-community<0.2.0\" \\\n",
        "  langchain-openai \\\n",
        "  langchain-experimental \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  torch \\\n",
        "  rank_bm25 \\\n",
        "  pymupdf \\\n",
        "  deepeval"
      ],
      "metadata": {
        "id": "6YXPDdbW__X4",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Gv8GHzJ2_kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cd69d3-6f28-49ed-a9e8-2d6457eeac3c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RAG_TECHNIQUES' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**helper_functions.py:**\n",
        "```\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain import PromptTemplate\n",
        "from openai import RateLimitError\n",
        "from typing import List\n",
        "from rank_bm25 import BM25Okapi\n",
        "import fitz\n",
        "import asyncio\n",
        "import random\n",
        "import textwrap\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "```"
      ],
      "metadata": {
        "id": "a9aXQwoBfDdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**evalute.py:**\n",
        "\n",
        "```\n",
        "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"gpt-4.1\",\n",
        "        base_url=base_url,\n",
        "        api_key=api_key,\n",
        "    )\n",
        "\n",
        "correctness_metric = GEval(\n",
        "  name=\"Correctness\",\n",
        "  model=\"gpt-4.1\",\n",
        "  evaluation_params=[\n",
        "    LLMTestCaseParams.EXPECTED_OUTPUT,\n",
        "    LLMTestCaseParams.ACTUAL_OUTPUT ],\n",
        "  evaluation_steps=[ \"Determine whether the actual output is factually correct based on the expected output.\" ],\n",
        "  )\n",
        "\n",
        "faithfulness_metric = FaithfulnessMetric(\n",
        "  threshold=0.7,\n",
        "  model=\"gpt-4.1\",\n",
        "  include_reason=False\n",
        ")\n",
        "\n",
        "relevance_metric = ContextualRelevancyMetric(\n",
        "  threshold=1,\n",
        "  model=\"gpt-4.1\",\n",
        "  include_reason=True\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lctPwBoufOhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "JTyFfvYfmMRR",
        "outputId": "c38beeb3-32be-4755-9f64-fac7feaa7c67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.2.0 (from langchain-text-splitters)\n",
            "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters)\n",
            "  Downloading langsmith-0.6.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n",
            "Downloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.6.4-py3-none-any.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain-core, langchain-text-splitters\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.87\n",
            "    Uninstalling langsmith-0.0.87:\n",
            "      Successfully uninstalled langsmith-0.0.87\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.23\n",
            "    Uninstalling langchain-core-0.1.23:\n",
            "      Successfully uninstalled langchain-core-0.1.23\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.0.6 requires langchain-core<0.2,>=0.1.16, but you have langchain-core 1.2.7 which is incompatible.\n",
            "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 1.2.7 which is incompatible.\n",
            "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.6.4 which is incompatible.\n",
            "langchain 0.1.1 requires langchain-core<0.2,>=0.1.9, but you have langchain-core 1.2.7 which is incompatible.\n",
            "langchain 0.1.1 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.6.4 which is incompatible.\n",
            "langchain-experimental 0.0.49 requires langchain-core<0.2.0,>=0.1.7, but you have langchain-core 1.2.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.2.7 langchain-text-splitters-1.1.0 langsmith-0.6.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core",
                  "langsmith"
                ]
              },
              "id": "8e977fe34cb245a0b29d85c8c6f958bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "On6wZ1c92_kU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "# from dotenv import load_dotenv\n",
        "# Load environment variables from a .env file\n",
        "# load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('key_ptn')\n",
        "os.environ[\"HF_KEY\"] = userdata.get('key_hf')\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://llm.ptnglobalcorp.com\"\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nClc5sUR2_kU"
      },
      "source": [
        "### Define the document's path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-5zT0HQI2_kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "aeba0202-ecad-442d-c5d6-a0fcf7bdc06f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-26 10:00:58--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "data/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2026-01-26 10:00:58 (14.2 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2026-01-26 10:00:58--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "data/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2026-01-26 10:00:59 (19.7 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z2TRsd8T2_kV"
      },
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"gpt-4.1\",\n",
        "        base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    )\n",
        "print(llm.invoke(\"I love programming.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AJcIFujqEU5",
        "outputId": "ce68feb3-4e83-4762-d1af-360910d90713"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='That’s awesome! Programming is a fantastic skill—it lets you build things, solve problems, and express creativity. What languages or projects are you working on, or is there something specific you enjoy most about programming?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tspxOhfBoqLO",
        "outputId": "6c56720d-a4fc-488c-9628-f9d5be75d59c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.6.1-py3-none-any.whl (328 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGUB3w1S2_kV"
      },
      "source": [
        "### Create a vector store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "BT0R_U_hwpDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AIGozJ4m2_kV"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents\n",
        "\n",
        "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "vectorstore = encode_pdf(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jC4ktqj2_kW"
      },
      "source": [
        "## Method 1: LLM based function to rerank the retrieved documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8LmcgUp2_kW"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/rerank_llm.svg?raw=1\" alt=\"rerank llm\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adp-LAg82_kW"
      },
      "source": [
        "### Create a custom reranking function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_9s2ssjd2_kW"
      },
      "outputs": [],
      "source": [
        "class RatingScore(BaseModel):\n",
        "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
        "\n",
        "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"query\", \"doc\"],\n",
        "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
        "        Query: {query}\n",
        "        Document: {doc}\n",
        "        Relevance Score:\"\"\"\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
        "\n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
        "        score = llm_chain.invoke(input_data).relevance_score\n",
        "        try:\n",
        "            score = float(score)\n",
        "        except ValueError:\n",
        "            score = 0  # Default score if parsing fails\n",
        "        scored_docs.append((doc, score))\n",
        "\n",
        "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "    return [doc for doc, _ in reranked_docs[:top_n]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WFAs2RaR16Bq",
        "outputId": "962a13e7-5ca5-4f19-ffc9-01ef74136e42"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.45)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (3.13.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.0.20)\n",
            "Collecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (4.12.1)\n",
            "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.1.0)\n",
            "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "Installing collected packages: langsmith, langchain-core\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.6.4\n",
            "    Uninstalling langsmith-0.6.4:\n",
            "      Successfully uninstalled langsmith-0.6.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.7\n",
            "    Uninstalling langchain-core-1.2.7:\n",
            "      Successfully uninstalled langchain-core-1.2.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 1.1.0 requires langchain-core<2.0.0,>=1.2.0, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langgraph-prebuilt 1.0.6 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langgraph-checkpoint 4.0.0 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.1.23 langsmith-0.0.87\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core",
                  "langsmith"
                ]
              },
              "id": "aca45ea7eddc4628a561fa1a374032d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# NOTE: Do NOT import PydanticOutputParser or RatingScore here.\n",
        "# We will use standard Python text processing instead.\n",
        "\n",
        "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Reranks documents based on relevance to the query using a custom LLM model.\n",
        "    Uses Regex parsing to avoid 'langchain_core.beta' errors.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Configure the LLM\n",
        "    # Make sure to use the exact model name and base_url your API requires\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model_name=\"gpt-4.1\",       # Your required custom model name\n",
        "        openai_api_base=os.environ[\"OPENAI_BASE_URL\"],\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"],         # Your API Key\n",
        "        max_tokens=100              # We only need a short answer (the score)\n",
        "    )\n",
        "\n",
        "    # 2. Simple Prompt: Ask for a NUMBER only\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=\"\"\"You are a relevance ranking assistant.\n",
        "        Rate the relevance of the following document to the query on a scale from 0 to 10.\n",
        "\n",
        "        Query: {query}\n",
        "        Document: {doc}\n",
        "\n",
        "        IMPORTANT: Return ONLY a single number (e.g., 8.5). Do not write any explanations.\n",
        "        Score:\"\"\",\n",
        "        input_variables=[\"query\", \"doc\"]\n",
        "    )\n",
        "\n",
        "    # 3. Create the Chain (Prompt -> LLM)\n",
        "    chain = prompt_template | llm\n",
        "\n",
        "    scored_docs = []\n",
        "    print(f\"Scoring {len(docs)} documents using Regex method...\")\n",
        "\n",
        "    for doc in docs:\n",
        "        try:\n",
        "            # Invoke the chain\n",
        "            response = chain.invoke({\"query\": query, \"doc\": doc.page_content})\n",
        "\n",
        "            # Extract content safely (Handles different LangChain response objects)\n",
        "            if hasattr(response, 'content'):\n",
        "                content = response.content\n",
        "            else:\n",
        "                content = str(response)\n",
        "\n",
        "            # 4. ROBUST PARSING: Find the number using Regex\n",
        "            # This looks for patterns like \"8.5\", \"10\", \"Score: 7\" inside the text\n",
        "            match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", content)\n",
        "\n",
        "            if match:\n",
        "                score = float(match.group())\n",
        "            else:\n",
        "                score = 0.0 # Default to 0 if no number found\n",
        "\n",
        "            # Save score to metadata\n",
        "            doc.metadata[\"rerank_score\"] = score\n",
        "            scored_docs.append((doc, score))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document: {e}\")\n",
        "            scored_docs.append((doc, 0.0))\n",
        "\n",
        "    # 5. Sort by score (Highest first)\n",
        "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return top N documents\n",
        "    return [doc for doc, _ in reranked_docs[:top_n]]\n",
        "\n",
        "\n",
        "# --- TEST BLOCK ---\n",
        "# Run this immediately to verify the fix\n",
        "if 'initial_docs' in locals() and len(initial_docs) > 0:\n",
        "    print(\"Testing rerank_documents...\")\n",
        "    try:\n",
        "        final_docs = rerank_documents(query, initial_docs)\n",
        "        print(\"\\n--- Success! Top Document ---\")\n",
        "        print(f\"Score: {final_docs[0].metadata.get('rerank_score')}\")\n",
        "        print(final_docs[0].page_content[:150] + \"...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Still failing: {e}\")\n",
        "else:\n",
        "    print(\"Please run the 'initial_docs = vectorstore.similarity_search(...)' step first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8fat52fySPM",
        "outputId": "9095b6a0-6ef1-4f9e-e7be-7870720794f5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing rerank_documents...\n",
            "Scoring 15 documents using Regex method...\n",
            "\n",
            "--- Success! Top Document ---\n",
            "Score: 10.0\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Fore...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dUBYfm2_kW"
      },
      "source": [
        "### Example usage of the reranking function with a sample query relevant to the document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ai-5D_tx2_kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "06dc5b6f-3771-4e52-dddf-1e65a1f22340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring 15 documents using Regex method...\n",
            "Top initial documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n",
            "\n",
            "Document 3:\n",
            "The economic costs of climate change include damage to infrastructure, reduced agricultural \n",
            "productivity, health care costs, and lost labor productivity. Extreme weather events, such as \n",
            "hurricanes a...\n",
            "Query: What are the impacts of climate change on biodiversity?\n",
            "\n",
            "Top reranked documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n",
            "\n",
            "Document 3:\n",
            "cultural perceptions. \n",
            "Youth Engagement \n",
            "Youth are vital stakeholders in climate action. Empowering young people through education, \n",
            "activism, and leadership opportunities can drive transformative cha...\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the impacts of climate change on biodiversity?\"\n",
        "initial_docs = vectorstore.similarity_search(query, k=15)\n",
        "reranked_docs = rerank_documents(query, initial_docs)\n",
        "\n",
        "# print first 3 initial documents\n",
        "print(\"Top initial documents:\")\n",
        "for i, doc in enumerate(initial_docs[:3]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"Top reranked documents:\")\n",
        "for i, doc in enumerate(reranked_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHkGzduD2_kX"
      },
      "source": [
        "### Create a custom retriever based on our reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "m9jfqu4m2_kX"
      },
      "outputs": [],
      "source": [
        "# Create a custom retriever class\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class CustomRetriever(BaseRetriever):\n",
        "\n",
        "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
        "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
        "        return rerank_documents(query, initial_docs, top_n=num_docs)\n",
        "\n",
        "\n",
        "# Create the custom retriever\n",
        "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "\n",
        "# Create an LLM for answering questions\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-4.1\",\n",
        "    openai_api_base=os.environ[\"OPENAI_BASE_URL\"],\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Create the RetrievalQA chain with the custom retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=custom_retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDzrPhE22_kX"
      },
      "source": [
        "### Example query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ljprt4XF2_kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f7e475-b729-4bd9-db74-5e3156196ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring 30 documents using Regex method...\n",
            "\n",
            "Question: What are the impacts of climate change on biodiversity?\n",
            "Answer: Climate change impacts biodiversity in several significant ways:\n",
            "\n",
            "1. **Shifting Habitat Ranges:** As temperatures rise and precipitation patterns change, many species are forced to move to new areas where conditions are more suitable. This can lead to changes in the geographic ranges of both plants and animals.\n",
            "\n",
            "2. **Changing Species Distributions:** Some species may expand their ranges, while others may contract or even disappear from certain areas. This alters the composition of ecosystems and can disrupt existing ecological relationships.\n",
            "\n",
            "3. **Loss of Biodiversity:** The inability of some species to adapt or migrate quickly enough can lead to population declines and extinctions, reducing overall biodiversity.\n",
            "\n",
            "4. **Disruption of Ecosystem Functions:** Changes in species composition can affect ecosystem services such as pollination, nutrient cycling, and water regulation, impacting the health and productivity of ecosystems.\n",
            "\n",
            "5. **Marine Ecosystem Vulnerability:** In marine environments, rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity. These changes can disrupt food webs, alter reproductive cycles, and threaten habitats like coral reefs.\n",
            "\n",
            "6. **Ecological Imbalance:** The combined effects of species loss, migration, and altered interactions can disrupt the balance of ecosystems, making them more vulnerable to further environmental changes and invasive species.\n",
            "\n",
            "In summary, climate change threatens biodiversity by altering habitats, shifting species distributions, reducing species numbers, and disrupting ecosystem functions and balance.\n",
            "\n",
            "Relevant source documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n"
          ]
        }
      ],
      "source": [
        "result = qa_chain({\"query\": query})\n",
        "\n",
        "print(f\"\\nQuestion: {query}\")\n",
        "print(f\"Answer: {result['result']}\")\n",
        "print(\"\\nRelevant source documents:\")\n",
        "for i, doc in enumerate(result[\"source_documents\"]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBraqFO2_kX"
      },
      "source": [
        "### Example that demonstrates why we should use reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C_o9aval2_kY",
        "outputId": "2009b815-4e07-46cc-9cfb-d7ef1c194a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Retrieval Techniques\n",
            "==================================\n",
            "Query: what is the capital of france?\n",
            "\n",
            "Baseline Retrieval Result:\n",
            "\n",
            "Document 1:\n",
            "The capital of France is huge.\n",
            "\n",
            "Document 2:\n",
            "The capital of France is great.\n",
            "\n",
            "Advanced Retrieval Result:\n",
            "Scoring 5 documents using Regex method...\n",
            "\n",
            "Document 1:\n",
            "The capital of France is great.\n",
            "\n",
            "Document 2:\n",
            "The capital of France is beautiful.\n"
          ]
        }
      ],
      "source": [
        "chunks = [\n",
        "    \"The capital of France is great.\",\n",
        "    \"The capital of France is huge.\",\n",
        "    \"The capital of France is beautiful.\",\n",
        "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower.\n",
        "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\",\n",
        "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
        "]\n",
        "docs = [Document(page_content=sentence) for sentence in chunks]\n",
        "\n",
        "\n",
        "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    print(\"Comparison of Retrieval Techniques\")\n",
        "    print(\"==================================\")\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    print(\"Baseline Retrieval Result:\")\n",
        "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
        "    for i, doc in enumerate(baseline_docs):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "    print(\"\\nAdvanced Retrieval Result:\")\n",
        "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "    advanced_docs = custom_retriever.get_relevant_documents(query)\n",
        "    for i, doc in enumerate(advanced_docs):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "\n",
        "query = \"what is the capital of france?\"\n",
        "compare_rag_techniques(query, docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TukE1YEr2_kY"
      },
      "source": [
        "## Method 2: Cross Encoder models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctl4O-h12_kY"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/rerank_cross_encoder.svg?raw=1\" alt=\"rerank cross encoder\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH_sTLjq2_kZ"
      },
      "source": [
        "### Define the cross encoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ryPjdlez2_kZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865df056-2e4f-462d-82df-dfe5bbb2e36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEncoderRetriever created successfully!\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Any\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.pydantic_v1 import Field # Use LangChain's internal Pydantic\n",
        "\n",
        "# 1. Inherit ONLY from BaseRetriever\n",
        "class CrossEncoderRetriever(BaseRetriever):\n",
        "\n",
        "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
        "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
        "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
        "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    # 2. Rename to _get_relevant_documents (note the underscore)\n",
        "    # The run_manager argument is required by the base class signature in newer versions\n",
        "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "        # Initial retrieval from vector store\n",
        "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
        "\n",
        "        # Basic check to avoid errors if no docs found\n",
        "        if not initial_docs:\n",
        "            return []\n",
        "\n",
        "        # Prepare pairs for cross-encoder (Query, Document Text)\n",
        "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
        "\n",
        "        # Get cross-encoder scores\n",
        "        scores = self.cross_encoder.predict(pairs)\n",
        "\n",
        "        # Zip docs with scores and sort\n",
        "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top K reranked documents\n",
        "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
        "\n",
        "# --- Usage ---\n",
        "\n",
        "# Initialize the CrossEncoder\n",
        "# (Make sure you have sentence-transformers installed)\n",
        "from sentence_transformers import CrossEncoder\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# Create the retriever\n",
        "retriever = CrossEncoderRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    cross_encoder=cross_encoder_model,\n",
        "    k=10,             # Fetch 10 candidates\n",
        "    rerank_top_k=3    # Return best 3\n",
        ")\n",
        "\n",
        "print(\"CrossEncoderRetriever created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QubZx8xO2_kZ"
      },
      "source": [
        "### Create an instance and showcase over an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3WbqdQS22_kZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80ea507-51ea-4c56-c3af-8ee2c86ca144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What are the impacts of climate change on biodiversity?\n",
            "Answer: Climate change has significant impacts on biodiversity, affecting both terrestrial and marine ecosystems. Here are the main impacts:\n",
            "\n",
            "**1. Shifts in Habitat Ranges and Species Distributions:**  \n",
            "Climate change causes many species to move to new areas as temperatures and precipitation patterns change. This can lead to shifts in habitat ranges and changes in where species are found.\n",
            "\n",
            "**2. Changes in Species Composition:**  \n",
            "Forests, grasslands, and deserts are experiencing changes in the types of plants and animals that live there. Some species may thrive, while others decline or disappear, leading to a loss of biodiversity and disruption of ecological balance.\n",
            "\n",
            "**3. Disruption of Ecosystem Functions:**  \n",
            "As species distributions and compositions change, the functions that ecosystems provide—such as pollination, water purification, and carbon storage—can be disrupted.\n",
            "\n",
            "**4. Loss of Biodiversity:**  \n",
            "The combined effects of shifting habitats, changing species distributions, and disrupted ecosystem functions can lead to a loss of biodiversity, with some species facing increased risk of extinction.\n",
            "\n",
            "**5. Impacts on Marine Ecosystems:**  \n",
            "Marine ecosystems are highly vulnerable. Rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity, from coral reefs to deep-sea habitats. These changes can disrupt marine food webs and fisheries.\n",
            "\n",
            "**6. Disruption of Reproductive Cycles and Food Webs:**  \n",
            "Climate change can alter the timing of reproductive cycles and migration patterns, which can disrupt food webs and the relationships between species.\n",
            "\n",
            "**7. Increased Vulnerability to Other Threats:**  \n",
            "Climate change can make ecosystems and species more vulnerable to other threats, such as invasive species, disease, and habitat destruction.\n",
            "\n",
            "**Summary:**  \n",
            "Overall, climate change threatens biodiversity by altering habitats, shifting species distributions, disrupting ecosystem functions, and increasing the risk of species loss and extinction. These impacts highlight the need for climate-resilient conservation strategies and the integration of biodiversity considerations into climate policies.\n",
            "\n",
            "Relevant source documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n",
            "\n",
            "Document 3:\n",
            "goals. Policies should promote synergies between biodiversity conservation and climate \n",
            "action. \n",
            "Chapter 10: Climate Change and Human Health \n",
            "Health Impacts \n",
            "Heat-Related Illnesses \n",
            "Rising temperature...\n",
            "\n",
            "Document 4:\n",
            "Local communities are often on the front lines of climate impacts and can be powerful agents \n",
            "of change. Community-based conservation projects involve residents in protecting and \n",
            "restoring natural re...\n",
            "\n",
            "Document 5:\n",
            "cultural perceptions. \n",
            "Youth Engagement \n",
            "Youth are vital stakeholders in climate action. Empowering young people through education, \n",
            "activism, and leadership opportunities can drive transformative cha...\n"
          ]
        }
      ],
      "source": [
        "# Create the cross-encoder retriever\n",
        "cross_encoder_retriever = CrossEncoderRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    cross_encoder=cross_encoder,\n",
        "    k=10,  # Retrieve 10 documents initially\n",
        "    rerank_top_k=5  # Return top 5 after reranking\n",
        ")\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-4.1\",\n",
        "    openai_api_base=os.environ[\"OPENAI_BASE_URL\"],\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Create the RetrievalQA chain with the cross-encoder retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=cross_encoder_retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Example query\n",
        "query = \"What are the impacts of climate change on biodiversity?\"\n",
        "result = qa_chain({\"query\": query})\n",
        "\n",
        "print(f\"\\nQuestion: {query}\")\n",
        "print(f\"Answer: {result['result']}\")\n",
        "print(\"\\nRelevant source documents:\")\n",
        "for i, doc in enumerate(result[\"source_documents\"]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0CB8kT2_kZ"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--reranking)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}